{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import pickle\n",
    "from scipy.linalg import svd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import gensim\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data, stopwords = None):\n",
    "    # Lower case\n",
    "    data = data.lower()\n",
    "    data = data.encode('ascii', 'ignore')\n",
    "\n",
    "    # Punctation removal\n",
    "    data = data.translate(None, string.punctuation) # changes to be done for python 3\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    token_list = tokenizer.tokenize(data)\n",
    "\n",
    "    # Removing stopwords\n",
    "    if stopwords is not None:\n",
    "        token_list = [word for word in token_list if word not in stopwords]\n",
    "\n",
    "    # Stemming\n",
    "#     stemmer = PorterStemmer()\n",
    "#     token_list = [stemmer.stem(word) for word in token_list]\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_document_term_matrix(data, root_folder, data_name, stop_words, k=1000):\n",
    "\n",
    "\tprint('Generating document term matrix for {0}....'.format(data_name))\n",
    "\ttoken_count_map = {}\n",
    "\n",
    "\t# vocabulary of k words based on frequency after stopword word removal, punctuation aremoval and stemming \n",
    "\tfor text in data.Text:\n",
    "\t\ttoken_list = tokenize(text, stop_words)\n",
    "\t\tfor token in token_list:\n",
    "\t\t\tif token in token_count_map:\n",
    "\t\t\t\ttoken_count_map[token] = token_count_map[token] + 1\n",
    "\t\t\telse:\n",
    "\t\t\t\ttoken_count_map[token] = 1\n",
    "\n",
    "\t# sort token_count_map decreasing order of count\n",
    "\tsorted_item_list = sorted(token_count_map.items(), key=lambda t: t[1], reverse=True)\n",
    "\tvocabulary = set([x[0] for x in sorted_item_list[0:k]])\n",
    "\n",
    "\ttfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize, min_df=1, analyzer=\"word\", stop_words=english_stops, vocabulary = vocabulary)\n",
    "\tdoc_term_sparse_mat = tfidf_vectorizer.fit_transform(data.Text)\n",
    "\n",
    "\t# saving document term matrix\n",
    "\twith open('{0}/{1}_doc_term_matrix_{2}.pkl'.format(root_folder,data_name, k), 'wb') as fp:\n",
    "\t\tpickle.dump(doc_term_sparse_mat, fp)\n",
    "\n",
    "\tprint('Finished generating document term matrix for {0}....'.format(data_name))\n",
    "\n",
    "\treturn doc_term_sparse_mat, tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_document_mat(mat , data_name, save_folder, vocab, xmin=0.4, xmax=0.8, ymin=-0.8, ymax=0.8):\n",
    "    plt.figure(figsize = (12,20))\n",
    "    plt.title('Data representation in reduced dimension: {0}'.format(data_name))\n",
    "    plt.xlabel('dim 1')\n",
    "    plt.ylabel('dim 2')\n",
    "    plt.xlim(xmin, xmax)\n",
    "    plt.ylim(ymin,ymax)\n",
    "    plt.grid()\n",
    "    plt.plot(mat[:, :1], mat[:, 1:], 'ro')\n",
    "    for i, txt in enumerate(range(mat.shape[0])):\n",
    "        plt.annotate(vocab[txt], (mat[:, :1][i],mat[:, 1:][i]))\n",
    "    #plt.savefig('{0}/{1}_dataplot.png'.format(save_folder, data_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main intializations\n",
    "english_stops = set(stopwords.words('english'))\n",
    "root_folder = '/home/vparambath/Desktop/iith/IR-Assignment2'\n",
    "data_folder = '/home/vparambath/Desktop/iith/IR-Assignment2'\n",
    "\n",
    "\n",
    "# Read data\n",
    "data = pd.read_csv('{0}/Dataset-2.txt'.format(data_folder), sep=':', header=None, names=['TextId', 'Text'], nrows =10000)\n",
    "doc_term_matrix, vocab = generate_document_term_matrix(data, root_folder, 'dataset2', english_stops)\n",
    "inv_vocab = {v: k for k, v in vocab.iteritems()}\n",
    "\n",
    "svd_5 = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "svd_2 = TruncatedSVD(n_components=2, n_iter=7, random_state=42)\n",
    "reduced_mat_2 = svd_2.fit_transform(doc_term_matrix.T)\n",
    "reduced_mat_5 = svd_5.fit_transform(doc_term_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/tirthajyoti/Interactive_Machine_Learning/blob/master/Curve_fit_widget_1.ipynb\n",
    "#plot_document_mat(reduced_mat_2, 'd2', root_folder, inv_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_document_mat(mat , data_name, save_folder, vocab):\n",
    "    @interact(xmin=(0,10,0.1), xmax=(0,10,0.1), ymin=(-10,10,0.1),ymax=(-10,10,0.1))\n",
    "    def plot_interact(xmin,xmax, ymin,ymax):\n",
    "        plt.figure(figsize = (12,10))\n",
    "        plt.title('Data representation in reduced dimension: {0}'.format(data_name))\n",
    "        plt.xlabel('dim 1')\n",
    "        plt.ylabel('dim 2')\n",
    "        plt.xlim(xmin, xmax)\n",
    "        plt.ylim(ymin,ymax)\n",
    "        plt.grid()\n",
    "        plt.plot(mat[:, :1], mat[:, 1:], 'ro')\n",
    "        for i, txt in enumerate(range(mat.shape[0])):\n",
    "            plt.annotate(vocab[txt], (mat[:, :1][i],mat[:, 1:][i]))\n",
    "        #plt.savefig('{0}/{1}_dataplot.png'.format(save_folder, data_name))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_document_mat(reduced_mat_2, 'd2', root_folder, inv_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(reduced_mat_2)\n",
    "\n",
    "cluster_range = range( 1, 20 )\n",
    "cluster_errors = []\n",
    "for num_clusters in cluster_range:\n",
    "    clusters = KMeans( num_clusters )\n",
    "    clusters.fit( X_scaled )\n",
    "    cluster_errors.append( clusters.inertia_ )\n",
    "    \n",
    "clusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\")\n",
    "plt.xticks(np.arange(1,20), np.arange(1,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=6)\n",
    "kmeans.fit(X_scaled)\n",
    "y_kmeans = kmeans.predict(X_scaled)\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)\n",
    "# for i, txt in enumerate(range(X_scaled.shape[0])):\n",
    "#         plt.annotate(txt, (X_scaled[:, 0][i], X_scaled[:, 1][i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/vparambath/Desktop/iith/IR-Assignment2/dataset1_doc_term_matrix_50000.pkl','rb')as fp:\n",
    "    doc_term_matrix = pickle.load(fp)\n",
    "    \n",
    "with open('/home/vparambath/Desktop/iith/IR-Assignment2/dataset1_vocabulary_50000.pkl','rb')as fp:\n",
    "    vocab = pickle.load(fp)\n",
    "    \n",
    "# print(doc_term_matrix.shape)\n",
    "# print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1e7b16515d41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparse2Corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_term_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/iith/python3/lib/python3.5/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/iith/python3/lib/python3.5/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    718\u001b[0m                         \u001b[0mpass_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_no\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m                     )\n\u001b[0;32m--> 720\u001b[0;31m                     \u001b[0mgammat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_estep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_alpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/iith/python3/lib/python3.5/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mdo_estep\u001b[0;34m(self, chunk, state)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumdocs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# avoids calling len(chunk) on a generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/iith/python3/lib/python3.5/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0mElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m                 \u001b[0mexpElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mElogthetad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m                 \u001b[0mphinorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpElogthetad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpElogbetad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m                 \u001b[0;31m# If gamma hasn't changed much, we're done.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m                 \u001b[0mmeanchange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlastgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lda_model = lda(gensim.matutils.Sparse2Corpus(doc_term_matrix, documents_columns=False), num_topics=5, id2word = {v: k for k, v in vocab.items()}, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_model.print_topics(num_topics=5, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.show_topic(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select top 50 words for each of the 5 LDA topics\n",
    "top_words = [[_ for _, word in lda_model.show_topic(topicno, topn=50)] for topicno in range(lda_model.num_topics)]\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/45310925/how-to-get-a-complete-topic-distribution-for-a-document-using-gensim-lda\n",
    "corpus = gensim.matutils.Sparse2Corpus(doc_term_matrix, documents_columns=False)\n",
    "[prob for _, prob in lda_model.get_document_topics(corpus[0])][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(9001)\n",
    "docs = list(range(len(corpus)))\n",
    "random.shuffle(docs)\n",
    "docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/22433884/python-gensim-how-to-calculate-document-similarity-using-the-lda-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "doc_topic_dist = []\n",
    "for i in range(len(corpus)):\n",
    "    dist = [prob for _, prob in lda_model.get_document_topics(corpus[i])]\n",
    "    if len(dist) == num_topics:\n",
    "        doc_topic_dist.append(np.array(dist))\n",
    "doc_topic_dist = np.array(doc_topic_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = [prob for _, prob in lda_model.get_document_topics(corpus[22996])]\n",
    "print(dist)\n",
    "np.array(dist).reshape(-1,5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jensen_shannon(query, matrix):\n",
    "    \"\"\"\n",
    "    This function implements a Jensen-Shannon similarity\n",
    "    between the input query (an LDA topic distribution for a document)\n",
    "    and the entire corpus of topic distributions.\n",
    "    It returns an array of length M where M is the number of documents in the corpus\n",
    "    \"\"\"\n",
    "     # take transpose\n",
    "    p = query[None,:].T\n",
    "     # transpose matrix\n",
    "    q = matrix.T\n",
    "    m = 0.5*(p + q)\n",
    "    return np.sqrt(0.5*(entropy(p,m) + entropy(q,m)))\n",
    "\n",
    "\n",
    "def get_most_similar_documents(query, matrix, k=5):\n",
    "\t\"\"\"\n",
    "    This function implements the Jensen-Shannon distance above\n",
    "    and retruns the top k indices of the smallest jensen shannon distances,\n",
    "    The smaller the Jensen-Shannon Distance, the more similar two distributions are\n",
    "    \"\"\"\n",
    "    # list of jensen shannon distances\n",
    "\tsims = jensen_shannon(query,matrix)\n",
    "\t# the top k positional index of the smallest Jensen Shannon distances\n",
    "\treturn sims.argsort()[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "def JSD(P, Q):\n",
    "    _P = P / norm(P, ord=1)\n",
    "    _Q = Q / norm(Q, ord=1)\n",
    "    _M = 0.5 * (_P + _Q)\n",
    "    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsd(p, q, base=np.e):\n",
    "    '''\n",
    "        Implementation of pairwise `jsd` based on  \n",
    "        https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\n",
    "    '''\n",
    "    p = np.asarray(p)\n",
    "    q = np.asarray(q)\n",
    "    m = 1./2*(p + q)\n",
    "    return sp.stats.entropy(p,m, base=base)/2. + sp.stats.entropy(q, m, base=base)/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSD(query, doc_topic_dist[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.spatial.distance import euclidean, pdist, squareform\n",
    "\n",
    "dists = pdist(doc_topic_dist, JSD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3_Python_3",
   "language": "python",
   "name": "jupyter3_python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
